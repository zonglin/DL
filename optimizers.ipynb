{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diagonal(x):\n",
    "    \"\"\"\n",
    "    Converts a vector into an diagonal matrix\n",
    "    \"\"\"\n",
    "    m = np.zeros((len(x),len(x)))\n",
    "    for i in range(len(m[0])):\n",
    "        m[i,i]=x[i]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X,axis=-1,order=2):\n",
    "    \"\"\"\n",
    "    Normalize the dataset X\n",
    "    \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(X,order,axis))\n",
    "    l2[l2 == 0] = 1##??\n",
    "    return X/np.expand_dims(l2,axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent():\n",
    "    def __init__(self,learning_rate=0.01,momentum=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.w_updt = None\n",
    "    def update(self,w,grad_wrt_w):\n",
    "        #if not initialized\n",
    "        if self.w_updt is None:\n",
    "            self.w_updt = np.zeros(np.shape(w))\n",
    "        #Use momentum is set\n",
    "        self.w_updt = self.momentum*self.w_updt + (1-self.momentum)*grad_wrt_w\n",
    "        #Move against the gradient to minimize loss\n",
    "        return w - self.learning_rate*self.w_updt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad():\n",
    "    def __init__(self,learning_rate = 0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.G = None#Sum of squares of the gradients\n",
    "        self.eps = 1e-8\n",
    "    def update(self,w,grad_wrt_w):\n",
    "        #If not initialized\n",
    "        if self.G is None:\n",
    "            self.G = np.zeros(np.shape(w))\n",
    "        #Add the square of the gradient of the loss function at w\n",
    "        self.G += np.power(grad_wrt_w,2)\n",
    "        #Adaptive gradient with higher learning rate for sparse data\n",
    "        return w - self.learning_rate * grad_wrt_w/np.sqrt(self.G+self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self,learning_rate=0.001,b1=0.9,b2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = 1e-8\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        #Delay rates\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "    def update(self,w,grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros(np.shape(grad_wrt_w))\n",
    "            self.v = np.zeros(np.shape(grad_wrt_w))\n",
    "        self.m = self.b1 * self.m + (1-self.b1) * grad_wrt_w\n",
    "        self.v = self.b2 * self.v + (1-self.b2) * grad_wrt_w\n",
    "        \n",
    "        m_hat = self.m / (1 - self.b1)\n",
    "        v_hat = self.v / (1 - self.b2)\n",
    "        \n",
    "        self.w_updt = self.learning_rate * m_hat/(np.sqrt(v_hat) + self.eps)\n",
    "        return w - self.w_updt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin(python3)",
   "language": "python",
   "name": "jin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
